{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 6001 - 2.3: A Review of Basic Linear Algebra\n",
    "\n",
    "\n",
    "### By the End Of This Lecture, you Will Be Able To:\n",
    "\n",
    "1. Identify the key areas of knowledge required for your first test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector and Matrix Rules\n",
    "\n",
    "It's best to think of vectors and matrices as being directly related. One way of thinking about it is considering a vector to be essentially a single-row matrix. Column vectors can be then compared to columns of a matrix and row vectors can be compared to rows.\n",
    "\n",
    "### Additive Rules of Matrices\n",
    "\n",
    "For three matrices $\\textit{A}$, $\\textit{B}$, and $\\textit{C}$ we have the following properties\n",
    "\n",
    "1. Commutative Law of Addition: $\\textit{A} + \\textit{B} = \\textit{B} + \\textit{A}$\n",
    "\n",
    "2. Associative Law of Addition: $(\\textit{A} + \\textit{B}) + \\textit{C} = \\textit{A} + (\\textit{B} + \\textit{C})$\n",
    "\n",
    "3. Associative Law of Multiplication: $\\textit{A}(\\textit{B}\\textit{C}) = (\\textit{A}\\textit{B})\\textit{C}$\n",
    "\n",
    "4. Distributive Law: $\\textit{A}(\\textit{B} + \\textit{C}) = \\textit{A}\\textit{B} + \\textit{A}\\textit{C}$\n",
    "\n",
    "5. Identity: There is the matrix equivalent of one. We define a matrix $\\textit{I_n}$ of dimension  $n \\times n$ such that the elements of $\\textit{I_n}$ are all zero, except the diagonal elements $i=j$; where $i_{ii} = 1$\n",
    "\n",
    "6. Zero: We define a matrix 0 of $m \\times n$ dimension as the matrix where all components $ij$ are 0\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$I_3 = \\begin{bmatrix}\n",
    "    1 & 0 & 0\\\\\n",
    "    0 & 1 & 0 \\\\\n",
    "    0 & 0 & 1\\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Here we can write $\\textit{I}\\textit{B} = \\textit{B}\\textit{I} = \\textit{B}$ or \n",
    "\n",
    "$\\textit{I}\\textit{I} = \\textit{I}$\n",
    "\n",
    "### Matrix Transpose\n",
    "\n",
    "The **transpose** of a matrix $\\textit{A}$ is formed by *interchanging* the rows and columns of $\\textit{A}$. That is \n",
    "\n",
    "$a_{ij}^T = a_{ji}$\n",
    "\n",
    "### Vector Multiplication\n",
    "\n",
    "Vectors have several ways they can be multiplied. In this program, we normally think of vectors as single-row or single-column matrices. Very frequently, we will see column vectors used with matrices. \n",
    "\n",
    "If we have two vectors:\n",
    "\n",
    "${\\bf{x}} = (x_1, x_2, ... , x_k)$ and ${\\bf{y}} = (y_1,y_2,...,y_k)$\n",
    "\n",
    "The inner product is written:\n",
    "\n",
    "$${\\bf{x}} \\cdot {\\bf{y}} = x_{1}y_{1}+x_{2}y_{2}+\\cdots+x_{k}y_{k}$$\n",
    "\n",
    "Therefore this produces a single *scalar* value.\n",
    "\n",
    "Following suit with the vectors-as-matrices trope, we can also write:\n",
    "\n",
    "${\\bf{x}} \\cdot {\\bf{y}} = {\\bf{x}^{T}}{\\bf{y}}$ =\n",
    "\n",
    "$ \\begin{bmatrix}x_1 & x_2 & \\cdots & x_k\\end{bmatrix} \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_k\\end{bmatrix}$\n",
    "\n",
    "\n",
    "\n",
    "### Matrix Multiplication\n",
    "\n",
    "When two matrices $\\bf{A}$ and $\\bf{B}$ are multiplied together we can write the product $\\bf{C}$ in *closed form* \n",
    "\n",
    "$\\bf{C} = \\bf{A}\\bf{B}$, whose elements $c_{ij}$ are given by addition of each component of the two matrices,  $c_{ij} = \\sum_{k=1}a_{ik}b_{kj}$. When done by hand, this process is performed like a vector dot product, multiplying each element in the left matrix's row $i$ by the right matrix's column $j$ and summing. This product becomes the entry $c_{ij}$.\n",
    "\n",
    "#### Example:\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "    5 & -3 \\\\\n",
    "    0 & -4 \\\\\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "    -2 & 0 \\\\\n",
    "    5 & 6 \\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    -25 & -18 \\\\\n",
    "    -20 & -24 \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Matrix multiplication is **not commutative**, that is\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "    -2 & 0 \\\\\n",
    "    5 & 6 \\\\\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "    5 & -3 \\\\\n",
    "    0 & -4 \\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    -10 & 6 \\\\\n",
    "    25 & -39 \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Remember: if you've a computer handy, you can always check your results with numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-10   6]\n",
      " [ 25 -39]]\n",
      "[[-25 -18]\n",
      " [-20 -24]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "A = np.asarray([[-2,0],[5,6]])\n",
    "B = np.asarray([[5,-3],[0,-4]])\n",
    "print A.dot(B)\n",
    "print B.dot(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUIZ: \n",
    "Write a matrix multiplication function of any size using for loops in Python:\n",
    "\n",
    "`def matrix_multiply(A, B):\n",
    "    #check to see if A and B are both np arrays\n",
    "    # check that the dimensions of each matrix are compatible\n",
    "    # you need to create C\n",
    "    for row in enumerate(A):\n",
    "        for column in enumerate(B.T):\n",
    "            #something goes here\n",
    "    return C`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Properties of Matrix Multiplication\n",
    "\n",
    "These are listed in the book on pp. 321:\n",
    "\n",
    "1. Matrix multiplication is not commutative (you know this), thus ${\\bf{AB}} \\neq {\\bf{BA}}$.\n",
    "2. ${\\bf{AB}} = 0$ does **not** imply ${\\bf{A}}=0$, ${\\bf{B}}=0$ or ${\\bf{BA}}=0$ !!! **EXCEPTION:** When ${\\bf{A}}$, ${\\bf{B}}$, ${\\bf{C}}$ are $n \\times n$ matrices (square) **and** $rank\\ {\\bf{A}}=n$, then it is implied ${\\bf{B}}=0$.\n",
    "3. ${\\bf{AC}}={\\bf{AD}}$ does **not** imply ${\\bf{C}}={\\bf{D}}$. **EXCEPTION:** When ${\\bf{A}}$, ${\\bf{B}}$, ${\\bf{C}}$ are $n \\times n$ matrices (square) **and** $rank\\ {\\bf{A}}=n$ (square, linearly independent, invertible).\n",
    "4. When ${\\bf{A}}$ is singular (noninvertible, zero determinant), then so are ${\\bf{AB}}$ and ${\\bf{BA}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Vector Spaces\n",
    "\n",
    "Recall the following vector space properties:\n",
    "\n",
    "   1. Closure under addition:  \n",
    "       $$\\text{If }{\\bf x},{\\bf y} \\in V, \\text{ then } {\\bf x} + {\\bf y} \\in V$$\n",
    "\n",
    "   2. Commutativity of vector addition:\n",
    "       $${\\bf x}+{\\bf y} = {\\bf y} + {\\bf x}$$\n",
    "\n",
    "   3. Associativity of vector addition:\n",
    "       $${\\bf x}+({\\bf y}+{\\bf z})=({\\bf x}+{\\bf y})+{\\bf z}$$\n",
    "       \n",
    "   4. Additive identity\n",
    "       $${\\bf 0}+{\\bf x} = {\\bf x} + {\\bf 0} = {\\bf x}$$\n",
    "       \n",
    "   5. Existence of additive inverse\n",
    "       $${\\bf x} + (- {\\bf x}) = {\\bf 0}$$\n",
    "       \n",
    "   6. Closure under scalar multiplication\n",
    "       $$\\text{for any scalar }a \\text{ and } {\\bf x}\\in V, a{\\bf x}\\in V$$ \n",
    "       \n",
    "   5. Associativity of scalar multiplication\n",
    "       $$a(b {\\bf x})=(ab){\\bf x}$$\n",
    "\n",
    "   6. Distributivity of scalar sums\n",
    "       $$(a+b){\\bf x} = a{\\bf x} + b {\\bf x}$$\n",
    "\n",
    "   7. Distributivity of vector sums\n",
    "       $$a({\\bf x} + {\\bf y})=a{\\bf x}+a {\\bf y}$$\n",
    "\n",
    "   8. Scalar multiplication identity\n",
    "       $$1 {\\bf x} = {\\bf x}$$\n",
    "       \n",
    "Although this seems like a lot to remember, the important thing is to realize that they function as a coherent set of rules to follow when dealing with vectors. They also reduce the notion of arrays of numbers into small symbols that can be coherently talked about in **closed form**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###QUIZ:\n",
    "\n",
    "What is the most commonly used vector space? Why do we prefer it over all others?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Linear Independence\n",
    "\n",
    "The previous discussion of linear independence leads us to a discussion of what it is. A set of vectors is **linearly independent** iff each vector cannot be constructed as a combination of the other vectors. Thus, each vector is *independent* from each other. \n",
    "\n",
    "The premier way to determine linear independence and thus the **rank** and **nullity** of a matrix is by performing Gauss elimination and back substitution.\n",
    "\n",
    "Another way to determine linear independence is by obtaining the **determinant** of the matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank, Span\n",
    "\n",
    "An idea integral to understanding vector spaces is the notion of a  *spanning set*. The basic concept is that we create a basis set of vectors that is capable of reproducing any other vector (and thus reaching every point) within a given vector space $V$. \n",
    "\n",
    "The best example of this is the standard basis set of dimension $n$,  $e_{n}$. This basis set is capable of reproducing any other point in $R^{n}$. \n",
    "\n",
    "Now that you know more about linear algebra, we can talk a bit about how to get a span and what takes place with it. \n",
    "\n",
    "Suppose we have a transformation $L$ in $R^3$:\n",
    "\n",
    "${\\bf{L}} = \\begin{bmatrix}1 & 0 & -1\\\\2 & 2 & -1 \\\\ 3 & 0 & -3 \\end{bmatrix}$\n",
    "\n",
    "The **span** of the transformation is obtained by determining the linearly independent vectors of it, such that we form a *basis*.\n",
    "\n",
    "First, produce the reduced row-eschelon form of the transformation so you can see its **rank**:\n",
    "\n",
    "${\\bf{L}} = \\begin{bmatrix}1 & 0 & -1\\\\0 & 1 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}$\n",
    "\n",
    "### QUIZ:\n",
    "What is the rank of $L$?\n",
    "\n",
    "\n",
    "Clearly, the system is not *Linearly Independent*. The entire space of the transformation can be *spanned* by two vectors:\n",
    "\n",
    "$span\\ L = c_{1}\\begin{bmatrix}1 \\\\ 0 \\\\ -1\\end{bmatrix} + c_{2}\\begin{bmatrix}0 \\\\ 1 \\\\ 0\\end{bmatrix}$\n",
    "\n",
    "\n",
    "You could go back and see that $L_{1}$ is $\\begin{bmatrix}1 \\\\ 0 \\\\ -1\\end{bmatrix}$ and $L_{3}$ is $\\begin{bmatrix}3 \\\\ 0 \\\\ -3\\end{bmatrix} = 3\\begin{bmatrix}1 \\\\ 0 \\\\ -1\\end{bmatrix}$\n",
    "\n",
    "\n",
    "The **range** or **image** of this transformation is the set of column vectors spanned by it, as if it were ${\\bf{Lx}}$:\n",
    "\n",
    "$image\\ L = x_{1}\\begin{bmatrix} 1 \\\\2 \\\\ 3 \\end{bmatrix} +  x_{2}\\begin{bmatrix} 1 \\\\2 \\\\ 3 \\end{bmatrix} +  x_{3}\\begin{bmatrix} 1 \\\\2 \\\\ 3 \\end{bmatrix}$\n",
    "\n",
    "Thus the image denotes all possible values the transformation could take, admitting any possible value of $\\bf x$\n",
    "\n",
    "### QUIZ:\n",
    "\n",
    "This leads us to another very important topic. How do we know that a set is a spanning set of a vector space $V$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Inverse of a Matrix\n",
    "\n",
    "Getting the inverse of a matrix is very important, because much of the processes used in linear algebra, particularly the design of equations to compute a needed value, is based on matrix invertibility.\n",
    "\n",
    "\n",
    "### The shared relationship of linear independence\n",
    "There is a shared relationship amongst determinants, inverses, and linear independence.\n",
    "1. If a matrix has a nonzero determinant, the linear system it characterizes has a determined solution.\n",
    "2. If a transformation is invertible, its matrix will have a nonzero determinant.\n",
    "3. If a transformation spans its vector space, its span is linearly independent and has the same rank as the vector space's dimension.\n",
    "\n",
    "### QUIZ:\n",
    "How do you know if a matrix is invertible? What's the fastest way to get that information?\n",
    "\n",
    "\n",
    "### Obtaining the Inverse\n",
    "The inverse of an invertible matrix should be obtainable by one of at least two methods (although there are more):\n",
    "\n",
    "1. Gauss-Jordan elimination.\n",
    "2. The general inverse method (inverse by cofactors)\n",
    "\n",
    "###QUIZ:\n",
    "Compute the inverse of $\\begin{bmatrix}2&6\\\\-2&1\\end{bmatrix}$ using both methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Solving the Eigenproblem\n",
    "\n",
    "The eigenproblem:\n",
    "\n",
    "$${\\bf A}{\\bf x} = \\lambda {\\bf x}$$\n",
    "\n",
    "is of tremendous importance. Sometimes you will see it posed as the \"general eigenproblem,\" which differs little from the standard formulation:\n",
    "\n",
    "$${\\bf A}{\\bf x} = {\\bf B}\\lambda {\\bf x}$$\n",
    "\n",
    "If ${\\bf B}$ is invertible this is exactly the same as the standard formulation.\n",
    "\n",
    "###QUIZ:\n",
    "Why do we say this here?\n",
    "\n",
    "-----\n",
    "\n",
    "You are responsible for being able to describe all features of the figure featured in lecture 2.2 in your own words. (reviewed in class)\n",
    "\n",
    "We typically solve the eigenproblem through the use of the **characteristic polynomial**, which is derived from posing the standard eigenproblem as an equation solved for $\\bf 0$ and computing its determinant:\n",
    "\n",
    "$$\\left|{\\bf A}-\\lambda\\right|{\\bf x}={\\bf 0}$$\n",
    "\n",
    "This is interesting. Why the use of a determinant here? We are producing a model wherein the hypothesis is that we are trying to find vectors **other than 0** that satisfy $({\\bf A}-\\lambda){\\bf x}={\\bf 0}$. This means that the hypothesized matrix $({\\bf A}-\\lambda)$ is **not** invertible. Hence, it must have a determinant of **0**.\n",
    "\n",
    "###QUIZ:\n",
    "Why does the characteristic equation require $({\\bf A}-\\lambda)$ to be singular?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##To Solve in Class:\n",
    "\n",
    "\n",
    "Given A = $\\begin{bmatrix}1 & 2 & -8 \\\\ 2 & 4 & 10 \\\\ 3 & 1 & 2 \\\\ 0 & 2 & 4 \\end{bmatrix}$\n",
    "and B = $\\begin{bmatrix} 2 & 0 & 1 & 1 \\\\ 4 & 1 & 2 & 0\\\\ 0 & 2 & 3 & 4\\end{bmatrix}$\n",
    "\n",
    "1. Does A have a determinant? Does B have a determinant? Are they invertible?\n",
    "2. Can we compute $\\bf{AB}$ What would this product be?\n",
    "3. Does $\\bf{AB^{-1}}$ exist?\n",
    "4. If $\\bf{AB}$ exists, does $\\bf{AB}^{T}$?\n",
    "5. $\\bf{AB}^{-1}$ what would this be in both closed and open form?\n",
    "6. What are the eigenvalues of $\\bf{BA}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.3.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
