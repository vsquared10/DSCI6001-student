{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 6001 5.4 Lab QR Factorization III\n",
    "\n",
    "### Householder Factorization\n",
    "\n",
    "\n",
    "## Rotation Strategies for QR Factorization\n",
    "\n",
    "\n",
    "The Gram-Schmidt algorithm is not terribly efficient for heavy repeated applications , however, and so **rotation strategies** are what are typically applied. These are either **Householder reflections** or **Givens rotations**. These strategies are typically employed in the \"Francis\" algorithm which is the current state of the art for square matrix decomposition.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A simple tweak is the usual way that the QR decomposition is presented in practice:\n",
    "\n",
    "Consider:\n",
    "\n",
    "$A = QR$\n",
    "\n",
    "now reverse the order of multiplication:\n",
    "\n",
    "$RQ = Q^{-1}AQ$\n",
    "\n",
    "It so happens, due to reasons not covered in this course, that multiple applications of the similarity transformation result in a final product $RQ$ that becomes upper triangular such that the eigenvalues can be read from the diagonal:\n",
    "\n",
    "$A = Q_{k}Q_{k-1}Q_{k-2}...Q_{1}R$\n",
    "\n",
    "The typical way this above formulation is done is by using **rotations**; either **Householder** or **Givens rotations.** We will only cover Householder rotations today due to time constraints required in developing an intuition for Givens rotations. You are encouraged to explore this on your own.\n",
    "\n",
    "## Householder reflections\n",
    "\n",
    "Householder reflections are a way of obtaining $Q$ implicitly without ever calculating the GS basis directly. They are efficient to calculate using common matrix operations and can be improved using modern techniques of broadcasting. They do not require explicit calculations of matrix products and require less in the way of storage and operations than any of the GS algorithms.\n",
    "\n",
    "### Householder reflections: Concepts\n",
    "\n",
    "Householder transformations are generalizations of reflections across the plane. These are matrices of the form:\n",
    "\n",
    "$$ H_{u} = I - 2uu^{T}$$\n",
    "\n",
    "Where $I$ is the identity matrix, and $u$ is an $N$ dimensional unit vector.\n",
    "\n",
    "H is symmetric: $(H_{u})^{T} = (I - 2uu^{T})^{T} = I^{T}-2(uu^{T})^{T} = I - 2uu^{T} = H_{u}$\n",
    "\n",
    "H is also orthogonal: $H_{u}^{T}H_{u} = (I - 2uu^{T})^{T}(I - 2uu^{T}) = I - 4uu^{T} + 4uu^{T}uu^{T} = I$\n",
    "\n",
    "When you apply $H_{u}$ to a target vector $\\bf{y}$: \n",
    "\n",
    "$H_{u}{\\bf{y}} = {\\bf{y}}-2uu^{T}{\\bf{y}}$\n",
    "\n",
    "This corresponds to reflecting ${\\bf{y}}$ about the line through the origin perpendicular to ${\\bf{u}}$ as shown in the below figure:\n",
    "\n",
    "![householder](./imgs/householder.png)\n",
    "\n",
    "Making use of the standard basis axes as a reference point we can choose\n",
    "\n",
    "$$ u_{1} = \\dfrac{{\\bf{y}} \\pm \\|{\\bf{y}}\\|e_{1}}{\\|{\\bf{y}} \\pm \\|{\\bf{y}}\\|e_{1}}e_{1}$$\n",
    "\n",
    "This produces a reflection along the $e_{1}$ axis. Then we must construct the Householder matrix:\n",
    "\n",
    "$$H_{u_{1}} = I - 2u_{1}u_{1}^{T}$$\n",
    "\n",
    "This matrix is applied in a rotation of $A$:\n",
    "\n",
    "$$ X_{1} = H_{u_{1}}A $$\n",
    "\n",
    "The basic idea is that we use the householder reflection to project ${\\bf{y}}$ onto the axis orthogonal to ${\\bf{u}}$. This projection is propagated throughout the matrix (of column vectors), effectively creating a change of coordinates (a sort of translation). This sets elements below the first diagonal to $0$, and provides a coordinate change to the remaining $n-1$ vectors. Then we take the next vector in the matrix and calculate its reflection against the previously corrected vector, setting the below elements to $0$, propagating to the $n-2$ vectors and so on.\n",
    "\n",
    "The householder algorithm proceeds for a $m \\times n$ matrix as follows:\n",
    "\n",
    "set $Q = I_{n}$\n",
    "\n",
    "$for\\ i\\ in\\ num\\ columns:$\n",
    "\n",
    "$\\ \\ \\ \\ \\ u_{i} = \\dfrac{{\\bf{y}} \\pm \\|{\\bf{y}}\\|e_{i}}{\\|{\\bf{y}} \\pm \\|{\\bf{y}}\\|e_{i}}e_{i}$\n",
    "\n",
    "$\\ \\ \\ \\ \\ H_{i} =  I - 2u_{i}u_{i}^{T}$\n",
    "\n",
    "$\\ \\ \\ \\ \\ Q = QH_{i}$\n",
    "\n",
    "Finally you end up with a series of $Q = Q_{n-1}Q_{n-2}...Q_{1}$, giving a final estimate for the real $Q$\n",
    "\n",
    "### Example:\n",
    "\n",
    "Let's get the $QR$ decomposition of the two-vector matrix \n",
    "\n",
    "$X = \\begin{bmatrix}1. & 1.26\\\\1. & 1.82\\\\1.& 2.22 \\end{bmatrix}$\n",
    "\n",
    "We first choose a $u$ to take the first column of the matrix to the x-axis:\n",
    "\n",
    "$u_{1} = \\begin{bmatrix}1.\\\\1.\\\\1.\\end{bmatrix}-\\sqrt{3}\\begin{bmatrix}1.\\\\0\\\\0\\end{bmatrix}$\n",
    "\n",
    "Then we need to normalize it:\n",
    "\n",
    "$u_{1} = \\dfrac{u_{1}}{\\|u_{1}\\|} =\\dfrac{1}{1.5925} \\begin{bmatrix}-0.7321.\\\\1.\\\\1.\\end{bmatrix}$\n",
    "\n",
    "Now create $H_{u_{1}}$:\n",
    "\n",
    "$H_{u_{1}} = I - 2u_{1}u_{1}^{T} = \\begin{bmatrix}1. & 0 & 0\\\\0 & 1. & 0\\\\0 & 0 & 1.\\end{bmatrix} - 2\\begin{bmatrix} 0.21132487 & -0.28867513 & -0.28867513\\\\-0.28867513 & 0.39433757 & 0.39433757\\\\ -0.28867513 & 0.39433757 & 0.39433757\\end{bmatrix}$\n",
    "\n",
    "$H_{u_{1}} = \\begin{bmatrix} 0.57735027 & 0.57735027 & 0.57735027 \\\\ 0.57735027 & 0.21132487 & -0.78867513\\\\ 0.57735027 & -0.78867513 & 0.21132487\\end{bmatrix}$\n",
    "\n",
    "And we'll start off the creation of Q by allowing the first $Q_{i}$ to be $H_{u_{1}}$:\n",
    "\n",
    "$Q = IH_{u_{1}}$\n",
    "\n",
    "\n",
    "The current state of the factorization can be taken with the matrix product of $H_{u_{1}}$ and $X$:\n",
    "\n",
    "$R = H_{u_{1}}X = \\begin{bmatrix} 1.7321 & 3.0600 \\\\ 0 & -0.6388 \\\\ 0 & -0.2388\\end{bmatrix} $\n",
    "\n",
    "Now we need to tear down the second column. Note that we don't want to lose the work we've done in the first row, so the next $u$ will not be considering the first row:\n",
    "\n",
    "$u_{2} = \\begin{bmatrix}0\\\\-0.6388\\\\-0.2388\\end{bmatrix}-0.6820\\begin{bmatrix}0\\\\1.\\\\0\\end{bmatrix}$\n",
    "\n",
    "$u_{2} = \\dfrac{u_{2}}{\\|u_{2}\\|} = \\dfrac{1}{1.3422}\\begin{bmatrix}0\\\\-1.3208\\\\-0.2388\\end{bmatrix}$\n",
    "\n",
    "$H_{u_{2}} = I - 2u_{2}u_{2}^{T} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & -2.48902528 & -0.63081408\\\\ 0 & -0.63081408 & 0.88594912\\end{bmatrix}$\n",
    "\n",
    "$R = H_{u_{2}}H_{u_{1}}X = \\begin{bmatrix} 1.7321 & 3.0600 \\\\ 0 & 0.6820 \\\\ 0 & 0\\end{bmatrix} $\n",
    "\n",
    "$Q = H_{u_{1}}H_{u_{2}} = \\begin{bmatrix} 0.57735027 & -0.74295879 & 0.33864273 \\\\\n",
    "  0.57735027 & 0.07820619 & -0.81274255 \\\\\n",
    "  0.57735027 & 0.6647526 & 0.47409982 \\end{bmatrix} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK:\n",
    "\n",
    "Below is given an example snippet of these operations as a major hint. Below that is the code stub you can use to fill out, or try a method of your own. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.21132487 -0.28867513 -0.28867513]\n",
      " [-0.28867513  0.39433757  0.39433757]\n",
      " [-0.28867513  0.39433757  0.39433757]]\n",
      "h1\n",
      "[[ 0.57735027  0.57735027  0.57735027]\n",
      " [ 0.57735027  0.21132487 -0.78867513]\n",
      " [ 0.57735027 -0.78867513  0.21132487]]\n",
      " \n",
      "[[ 0.57735027 -0.74295879  0.33864273]\n",
      " [ 0.57735027  0.07820619 -0.81274255]\n",
      " [ 0.57735027  0.6647526   0.47409982]]\n",
      " \n",
      "and the factorization emerges\n",
      "[[ 1.73205081  3.05995643]\n",
      " [ 0.          0.68195797]\n",
      " [ 0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "eps = 1.0E-10\n",
    "Q = np.eye(3)\n",
    "\n",
    "X = np.array([[1., 1.26],[1, 1.82],[1., 2.22]])\n",
    "R = np.copy(X)\n",
    "\n",
    "# we are making some shortcuts here, so as not to give everything away\n",
    "\n",
    "u1 = X[:,0]-LA.norm(X[:,0])*np.array([1.,0.,0.])\n",
    "u1 = u1/LA.norm(u1)\n",
    "\n",
    "H1 = np.identity(3)-2.*np.outer(u1, u1)\n",
    "\n",
    "print(np.outer(u1,u1))\n",
    "\n",
    "print(\"h1\")\n",
    "print(H1)\n",
    "print(' ')\n",
    "\n",
    "Q = np.dot(Q, H1)\n",
    "R = np.dot(H1, R)\n",
    "\n",
    "#continuing on, with more hints\n",
    "\n",
    "x = H1.T.dot(X)[1:,1] # now you ignore the top i rows (because they've already been solved)\n",
    "e = np.zeros_like(H1.T.dot(X)[1:,1]) # you want do reflection only on the elements that haven't been solved for yet\n",
    "e[0] = copysign(np.linalg.norm(x), -X[1, 1]) # this is a useful step \n",
    "                                            # just to make sure that you've got the right signed norm\n",
    "\n",
    "u = x+e\n",
    "v = u / np.linalg.norm(u)\n",
    "\n",
    "H_i = np.identity(3)\n",
    "H_i[1:, 1:] -= 2.0 * np.outer(v, v)\n",
    "R = np.dot(H_i, R)\n",
    "\n",
    "Q = np.dot(Q, H_i)\n",
    "\n",
    "# Here is a clean way to zero out low values\n",
    "low_values_indices = R < eps  # Where values are low\n",
    "R[low_values_indices] = 0  # All low values set to 0\n",
    "\n",
    "print(Q)\n",
    "print(' ')\n",
    "print('and the factorization emerges')\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.73,  0.  ,  0.  ])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LA.norm(X[:,0])\n",
    "1.73*np.array([1., 0., 0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import copysign\n",
    "\n",
    "def householder_reflection(A):\n",
    "    \"\"\"Perform QR decomposition of matrix A using Householder reflection.\"\"\"\n",
    "    (rows, cols) = np.shape(A)\n",
    "\n",
    "    # * Initialize orthogonal matrix Q and upper triangular matrix R.\n",
    "    Q = np.identity(rows) # * I would set Q = I\n",
    "    R = np.copy(A) # * I would set R=A but you'll need to make a copy of it.\n",
    "\n",
    "    \n",
    "    for i in range(rows - 1): # * iterate over each column subvector\n",
    "        x = R[i:, i] # * pick out the subvector we're looking at\n",
    "        print(\"i \", i, \"x\", x)        \n",
    "        \n",
    "        e = np.zeros_like(x) \n",
    "        e[0] = copysign(np.linalg.norm(x), -A[i, i]) # * get the correct sign and components of the subvector\n",
    "        \n",
    "        print(\"e: \", e)\n",
    "        \n",
    "        u = x + e # * build u from the subvector and the norm (there are several ways of doing this - look at the math)\n",
    "        \n",
    "        print(\"u:\", u)\n",
    "        \n",
    "        v = u / np.linalg.norm(u) # * norm this u\n",
    "\n",
    "        Q_i = np.identity(rows)\n",
    "        Q_i[i:, i:] -= 2.0 * np.outer(v, v) # * build Householder reflection\n",
    "        \n",
    "        print(\"Q[\",i,\"]\")\n",
    "        print(Q_i)\n",
    "\n",
    "        R = np.dot(Q_i, R) # Apply this householder reflection to R\n",
    "        Q = np.dot(Q, Q_i.T) # Apply this householder reflection to Q\n",
    "\n",
    "    return (Q, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i  0 x [-2.  1.  1.]\n",
      "e:  [ 2.44948974  0.          0.        ]\n",
      "u: [ 0.44948974  1.          1.        ]\n",
      "Q[ 0 ]\n",
      "[[ 0.81649658 -0.40824829 -0.40824829]\n",
      " [-0.40824829  0.09175171 -0.90824829]\n",
      " [-0.40824829 -0.90824829  0.09175171]]\n",
      "i  1 x [ 0.72474487  1.72474487]\n",
      "e:  [ 1.87082869  0.        ]\n",
      "u: [ 2.59557356  1.72474487]\n",
      "Q[ 1 ]\n",
      "[[ 1.          0.          0.        ]\n",
      " [ 0.         -0.38739243 -0.92191491]\n",
      " [ 0.         -0.92191491  0.38739243]]\n",
      "[[ 0.81649658  0.53452248  0.21821789]\n",
      " [-0.40824829  0.80178373 -0.43643578]\n",
      " [-0.40824829  0.26726124  0.87287156]]\n",
      "[[ -2.00000000e+00  -1.20335748e-15   1.00000000e+00]\n",
      " [  1.00000000e+00  -2.00000000e+00   1.00000000e+00]\n",
      " [  1.00000000e+00  -1.00000000e+00   1.02695630e-15]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "A = np.array([[-2, 0, 1 ],[1, -2, 1],[1, -1, 0]], dtype=float)\n",
    "(Q,R) = householder_reflection(A)\n",
    "#R = Q.T.dot(A)\n",
    "print(Q)\n",
    "print(Q.dot(R))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.3.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
